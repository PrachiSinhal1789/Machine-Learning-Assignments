{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc63077f",
   "metadata": {},
   "source": [
    "1. What is feature engineering, and how does it work? Explain the various aspects of feature engineering in depth.\n",
    "\n",
    "Ans.Feature engineering is a machine learning technique that leverages data to create new variables that aren't in the training set. It can produce new features for both supervised and unsupervised learning, with the goal of simplifying and speeding up data transformations while also enhancing model accuracy.\n",
    "Feature engineering refers to manipulation — addition, deletion, combination, mutation — of your data set to improve machine learning model training, leading to better performance and greater accuracy. Effective feature engineering is based on sound knowledge of the business problem and the available data sources\n",
    "\n",
    "2. What is feature selection, and how does it work? What is the aim of it? What are the various methods of function selection?\n",
    "\n",
    "Ans.Feature Selection is the method of reducing the input variable to your model by using only relevant data and getting rid of noise in data. It is the process of automatically choosing relevant features for your machine learning model based on the type of problem you are trying to solve.\n",
    "There are three categories of feature selection methods, depending on how they interact with the classifier, namely, filter, wrapper, and embedded methods.\n",
    "\n",
    "3. Describe the function selection filter and wrapper approaches. State the pros and cons of each approach?\n",
    "\n",
    "Ans.Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "\n",
    "4.\n",
    "\n",
    "i. Describe the overall feature selection process.\n",
    "\n",
    "Ans.Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most widely used function extraction algorithms?\n",
    "\n",
    "Ans.Feature extraction refers to the process of transforming raw data into numerical features that can be processed while preserving the information in the original data set. It yields better results than applying machine learning directly to the raw data.\n",
    "Most widely used function extraction algorithms are\n",
    "PCA, ICA, LDA, LLE, t-SNE and AE.\n",
    "\n",
    "5. Describe the feature engineering process in the sense of a text categorization issue.\n",
    "\n",
    "Ans.Feature engineering is one of the most important steps in machine learning. It is the process of using domain knowledge of the data to create features that make machine learning algorithms work.\n",
    " \n",
    "6.What makes cosine similarity a good metric for text categorization? A document-term matrix has two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in cosine.\n",
    "\n",
    "Ans.Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction. It is often used to measure document similarity in text analysis.\n",
    "0.675\n",
    "\n",
    "7.\n",
    "\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111, calculate the Hamming gap.\n",
    "\n",
    "Ans.Hamming distance is the number of values that are different between two vectors. It is typically used to compare two binary strings of equal length. It can also be used for strings to compare how similar they are to each other by calculating the number of characters that are different from each other.\n",
    "Hamming gap=2\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0, 0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n",
    "\n",
    "Ans.6/10\n",
    "\n",
    "8. State what is meant by  \"high-dimensional data set\"? Could you offer a few real-life examples? What are the difficulties in using machine learning techniques on a data set with many dimensions? What can be done about it?\n",
    "\n",
    "Ans.High-dimensional data are defined as data in which the number of features (variables observed), p, are close to or larger than the number of observations (or data points), n.\n",
    "eg.Data on health status of patients can be high-dimensional (100+ measured/recorded parameters from blood analysis, immune system status, genetic background, nutrition, alcohol- tobacco- drug-consuption, operations, treatments, diagnosed diseases, ...)\n",
    "\n",
    "The curse of dimensionality basically means that the error increases with the increase in the number of features. It refers to the fact that algorithms are harder to design in high dimensions and often have a running time exponential in the dimensions.\n",
    "There are two common ways to deal with high dimensional data:\n",
    "Choose to include fewer features. The most obvious way to avoid dealing with high dimensional data is to simply include fewer features in the dataset. ...\n",
    "Use a regularization method.\n",
    "\n",
    "9. Make a few quick notes on:\n",
    "\n",
    "1.PCA:The Principal Component Analysis is a popular unsupervised learning technique for reducing the dimensionality of data. It increases interpretability yet, at the same time, it minimizes information loss. It helps to find the most significant features in a dataset and makes the data easy for plotting in 2D and 3D.\n",
    "\n",
    "2. Use of vectors:Vectors are commonly used in machine learning as they lend a convenient way to organize data. Often one of the very first steps in making a machine learning model is vectorizing the data. They are also relied upon heavily to make up the basis for some machine learning techniques as well.\n",
    "\n",
    "3. Embedded technique:Embedded methods combine the qualities' of filter and wrapper methods. It's implemented by algorithms that have their own built-in feature selection methods. Some of the most popular examples of these methods are LASSO and RIDGE regression which have inbuilt penalization functions to reduce overfitting\n",
    "\n",
    "10. Make a comparison between:\n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection:Sequential forward selection (SFS), in which features are sequentially added to an empty candidate set until the addition of further features does not decrease the criterion.\n",
    "Backward elimination starts with all features included in the model and then removes the least relevant features one at a time. Forward selection starts with no features included in the model and then adds the most relevant features one at a time.\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper: Filter methods measure the relevance of features by their correlation with dependent variable while wrapper methods measure the usefulness of a subset of feature by actually training a model on it.\n",
    "\n",
    "3. SMC vs. Jaccard coefficient:The SMC counts both mutual presences (when an attribute is present in both sets) and mutual absence (when an attribute is absent in both sets) as matches and compares it to the total number of attributes in the universe.The Jaccard similarity index (sometimes called the Jaccard similarity coefficient) compares members for two sets to see which members are shared and which are distinct. It's a measure of similarity for the two sets of data, with a range from 0% to 100%. The higher the percentage, the more similar the two populations.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
