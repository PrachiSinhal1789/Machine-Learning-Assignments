{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d4eced6",
   "metadata": {},
   "source": [
    "1. What is the definition of a target function? In the sense of a real-life example, express the target function. How is a target function's fitness assessed?\n",
    "\n",
    "Ans.A target function, in machine learning, is a method for solving a problem that an AI algorithm parses its training data to find. Once an algorithm finds its target function, that function can be used to predict results (predictive analysis).\n",
    "We can access target function's fitness using metrics like r square,accuracy,precision,recall.\n",
    "\n",
    "2. What are predictive models, and how do they work? What are descriptive types, and how do you use them? Examples of both types of models should be provided. Distinguish between these two forms of models.\n",
    "\n",
    "Ans.Predictive analytics uses historical data to predict future events. Typically, historical data is used to build a mathematical model that captures important trends. That predictive model is then used on current data to predict what will happen next, or to suggest actions to take for optimal outcomes.\n",
    "Descriptive research aims to accurately and systematically describe a population, situation or phenomenon. It can answer what, where, when and how questions, but not why questions. A descriptive research design can use a wide variety of research methods to investigate one or more variables.\n",
    "Descriptive analytics reflect on things past consumer behavior such as:\n",
    "Customer purchase history\n",
    "The effectiveness of email or social media campaigns\n",
    "How webpages perform in terms of clicks, time on page, and conversions\n",
    "Descriptive analytics help companies understand what’s worked, what hasn’t, and what motivates their customers. Descriptive analytics take numbers and data about what has happened in the past, helping marketers identify patterns and trends.\n",
    "While descriptive analytics are used by companies to understand what has happened, predictive analytics are used by companies to determine what is likely to happen next. Supermetrics describes predictive analytics as “the process of using current and/or historical data with a combination of statistical techniques — including (but not limited to) data mining, predictive modeling, and machine learning — to assess the likelihood of a certain event happening in the future.”\n",
    "\n",
    "They go on to share five practical examples that give a thorough picture of predictive analytics in marketing. Those examples are:\n",
    "\n",
    "Customer and audience segmentation (using cluster modeling)\n",
    "New customer acquisition (using identification modeling)\n",
    "Lead scoring (using propensity modeling and predictive scoring)\n",
    "Content and ad recommendations (using collaborative filtering)\n",
    "Personalizing customer experiences (using automated segmentation)\n",
    "\n",
    "3. Describe the method of assessing a classification model's efficiency in detail. Describe the various measurement parameters.\n",
    "\n",
    "Ans.Accuracy,precision,recall,F beta score\n",
    "\n",
    "4. i. In the sense of machine learning models, what is underfitting? What is the most common reason for underfitting?\n",
    "      \n",
    "Ans.Your model is underfitting the training data when the model performs poorly on the training data. This is because the model is unable to capture the relationship between the input examples (often called X) and the target values (often called Y).\n",
    "\n",
    "ii. What does it mean to overfit? When is it going to happen?\n",
    "\n",
    "Ans.Overfitting is an undesirable machine learning behavior that occurs when the machine learning model gives accurate predictions for training data but not for new data.It happens when model learns too much training data.\n",
    "\n",
    "iii. In the sense of model fitting, explain the bias-variance trade-off.\n",
    "\n",
    "Ans.“Bias and variance are complements of each other” The increase of one will result in the decrease of the other and vice versa. Hence, finding the right balance of values is known as the Bias-Variance Tradeoff. Target Function. An ideal algorithm should neither underfit nor overfit the data.\n",
    "\n",
    "5. Is it possible to boost the efficiency of a learning model? If so, please clarify how.\n",
    "\n",
    "Ans.Ways to improve the accuracy of a model:\n",
    "1. Add more data\n",
    "2. Treat missing and Outlier values\n",
    "3. Feature Engineering\n",
    "4. Feature Selection\n",
    "5. Multiple algorithms\n",
    "6. Algorithm Tuning\n",
    "7. Ensemble methods\n",
    "\n",
    "6. How would you rate an unsupervised learning model's success? What are the most common success indicators for an unsupervised learning model?\n",
    "\n",
    "Ans.Adjusted Rand Index\n",
    "This is one of the variations of the classic Rand Index that tries to get the proportion of the cluster assignments which are “accurate.” It does so by computing the similarity measure between two different clusterings, taking into consideration all pairs of inputs and numbering those pairs that are assigned in the same or a different cluster, then comparing the same with the random probability of assignment of these clusters. The measuring metric is available in sklearn and can be directly used to create a quantifiable measure for clustering algorithms through the following clustering metrics:\n",
    "\n",
    "1.) Fowlkes-Mallows Score\n",
    "It is a bit similar in its results to the ARI as it also attempts to look at the cluster assignments that are accurate. This clustering metric computes the GM (Geometric Mean) between the precision and recall metric, and like most of the supervised learning measuring metrics, it is capped between the values of 0 – 1, with a larger value indicating that the algorithm did a good job at assigning individual samples to respective clusters. This is also available as a metric in sklearn and is therefore easy to implement.\n",
    "\n",
    "2.) Silhouette Score\n",
    "This algorithmtries to explain the extent to which two data points are similar to each other, given that they are assigned to the same cluster. The score is computed based on the aggregation of all the data points so we can have an average of the performance of the implementation in the entirety of the dataset. This metric measure lies between negative 1 and positive 1. Positive values are desired in this mechanism as a negative value indicates that the data points in the clusters are not very similar to each other.\n",
    "\n",
    "Calinski-Harabasz Index\n",
    "This implementation looks at the ratio of the variance of individual data points compared to the points in the clusters other than the one in which the current instance is attributed and the data points within the current cluster. For this metric, higher values are preferred.\n",
    "\n",
    "\n",
    "7. Is it possible to use a classification model for numerical data or a regression model for categorical data with a classification model? Explain your answer.\n",
    "\n",
    "Ans.Fundamentally, classification is about predicting a label and regression is about predicting a quantity. Why linear regression can't use for classification? The main reason for that is the predicted values are continuous, not probabilistic. So we can't get an exact class to accomplish the classification.\n",
    "Using classification model for numerical data we wont be able to predict exact numeric values.\n",
    "\n",
    "8. Describe the predictive modeling method for numerical values. What distinguishes it from categorical predictive modeling?\n",
    "\n",
    "Ans.Regression is used for numerical values.The most significant difference between regression vs classification is that while regression helps predict a continuous quantity, classification predicts discrete class labels.\n",
    "\n",
    "9. The following data were collected when using a classification model to predict the malignancy of a group of patients'tumors:\n",
    "         i. Accurate estimates – 15 cancerous, 75 benign\n",
    "         ii. Wrong predictions – 3 cancerous, 7 benign\n",
    "                Determine the model's error rate, Kappa value, sensitivity, precision, and F-measure.\n",
    "                \n",
    "Ans. Accuracy-90%\n",
    "    Kappa Value=1.259\n",
    "    Sensitivity=1/6\n",
    "    Precision=5/6\n",
    "    \n",
    "10. Make quick notes on:\n",
    "         1. The process of holding out-In the holdout method, data set is partitioned, such that – maximum data belongs to training set and remaining data belongs to test set. If there are 20 data items present, 12 are placed in training set and remaining 8 are placed in test set.\n",
    "         \n",
    "         2. Cross-validation by tenfold-With this method we have one data set which we divide randomly into 10 parts. We use 9 of those parts for training and reserve one tenth for testing. We repeat this procedure 10 times each time reserving a different tenth for testing.\n",
    "         \n",
    "         3. Adjusting the parameters-A Machine Learning model is defined as a mathematical model with a number of parameters that need to be learned from the data. By training a model with existing data, we are able to fit the model parameters. \n",
    "However, there is another kind of parameter, known as Hyperparameters, that cannot be directly learned from the regular training process. They are usually fixed before the actual training process begins. These parameters express important properties of the model such as its complexity or how fast it should learn. \n",
    "\n",
    "11. Define the following terms: \n",
    "         1. Purity vs. Silhouette width\n",
    "        Ans.Purity (external evaluation technique) evaluates the extent to which a cluster belongs to a class. It involves assigning a cluster to a class that is the most frequent in the cluster and then counting the number of correctly assigned data points per cluster, taking the sum over all clusters, and dividing the value by the total number of data points.\n",
    "        The silhouette coefficient (internal evaluation technique) is calculated for each data point using mean intra-cluster distance and mean inter-cluster distance.\n",
    "        \n",
    "         2. Boosting vs. Bagging\n",
    "       Ans.Bagging is a method of merging the same type of predictions. Boosting is a method of merging different types of predictions. Bagging decreases variance, not bias, and solves over-fitting issues in a model. Boosting decreases bias, not variance.\n",
    "       \n",
    "         3. The eager learner vs. the lazy learner\n",
    "Ans.In machine learning, lazy learning is a learning method in which generalization of the training data is, in theory, delayed until a query is made to the system, as opposed to eager learning, where the system tries to generalize the training data before receiving queries."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
